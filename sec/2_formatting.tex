\section{Methodology}
\label{sec:method}

In this section, we describe the three methods we employ to combine CNNs and RIC-CNNs:
%-------------------------------------------------------------------------
\subsection{Double Branch Model}

In this approach, we create a network with two branches: one with a standard CNN architecture, and the other with a RIC-CNN architecture. The idea is to allow each branch to learn different aspects of the input image. The CNN branch learns the detailed features, and the RIC-CNN branch learns the rotationally invariant features. The outputs of the two branches are then combined to make the final prediction.  In our implementation, the RIC-CNN branch is frozen for the first few epochs of training. This allows the CNN branch to initially learn basic features without being influenced by the RIC-CNN branch.

%-------------------------------------------------------------------------
\subsection{Take Best Confidence}

This approach is based on the intuition that when a rotated image is passed through a standard CNN, the CNN may not recognize the pattern if it has not seen that specific rotation during training. This leads to a more spread-out confidence score across different classes, resulting in a lower peak confidence.  RIC-CNN, on the other hand, is expected to produce a more reliable prediction with a higher peak confidence due to its rotational invariance.
\\ \\
Similarly, when an unrotated image is passed through an RIC-CNN, examples like label 6 and 9 in MNIST will look the same for RIC-CNN because of its rotational invariance property. This leads to a more spread-out confidence score across these classes, resulting in a lower peak confidence. In this case, standard CNN can clearly distinguish the difference between these labels and give more concentrated and higher peak confidence compared to RIC-CNN.
\\ \\
Therefore, in this approach, we take the prediction with the highest confidence score from either the CNN or the RIC-CNN to be the output.

\subsubsection{Notation}
Let $I \in \mathbb{R}^{H \times W \times C}$ be the input image, $\mathbf{y}_c \in \mathbb{R}^{C_l} = f_c(I, \hat{\theta}_c)$ and $\mathbf{y}_r \in \mathbb{R}^{C_l} = f_r(I, \hat{\theta}_r)$, where $\mathbf{y}_c$ and $\mathbf{y}_r$ are the prediction logits of the standard CNN $f_c$ and RIC-CNN $f_r$ respectively, where $C_l$ is the number of labels of the dataset of the classification task, with their corresponding weights $\hat{\theta}_c$ and $\hat{\theta}_r$.

\subsubsection{Independent Branch Training}
Both standard CNN branch and RIC-CNN branch are trained independently using the same datasets at the same time with the combined loss function
$$
\mathcal{L} = \mathcal{L}_{CE}(\sigma_s(\mathbf{y}_c), \mathbf{y}) + \mathcal{L}_{CE}(\sigma_s(\mathbf{y}_r), \mathbf{y})
$$
where $\mathcal{L}_{CE}$ denote the cross-entropy loss function, $\sigma_s$ denote the softmax function, and $\mathbf{y} \in \mathbb{R}^{C_l}$ is the one-hot representation of the ground truth label.
\\ \\
The \textit{Take Best Confidence} will not be used in the training stage to prevent training collapse to one model as only one branch of the model will the gradient of the loss flows through after taking maximum on confidence scores between the two.

\subsubsection{Take Best Confidence Prediction}
In prediction stage, both branches will contribute to the prediction to produce confidence scores $\mathbf{c}_c \in \mathbb{R}^{C_l}$ and $\mathbf{c}_r \in \mathbb{R}^{C_l}$ to each label class. We look for the maximum confidence score of each labels between the two branches, and select the label with the highest confidence as the final prediction output.
More specifically, we determine the prediction output $p$ by:
$$
p = \argmax_c(\mathbf{z}_c),\ \mathbf{z_i} = \max(\mathbf{c}_{c,i}, \mathbf{c}_{r,i}),\ \mathbf{c}_j = \sigma_s(\mathbf{y}_j),\ j \in {c, r}
$$
%-------------------------------------------------------------------------
\subsection{Feature Fusion}

RIC–CNN’s intermediate feature maps are rotation-equivariant: when the input is rotated, the RIC branch’s feature map $F_r$ rotates correspondingly before global averaging. In contrast, a standard CNN’s feature map $F_c$ remains rotationally variant. This mismatch between $F_r$ and $F_c$ encodes rich rotational information. To harness this, we propose a \emph{Feature Fusion} module that compares and dynamically weights these two feature maps to extract and leverage rotational cues for improved robustness.

\subsubsection{Notation}
Let $F_r\in\mathbb{R}^{H\times W\times C}$ be the feature map produced by the RIC branch and $F_c\in\mathbb{R}^{H\times W\times C}$ the feature map from the standard CNN branch at the same spatial resolution and channel dimensionality.

\subsubsection{Weight Generation}
We concatenate the two feature maps along the channel dimension and pass them through a $1\times1$ convolutional layer $g(\cdot)$, followed by a sigmoid activation to produce a spatially-varying weight map:
\[
W = \sigma\bigl(g([F_r \| F_c])\bigr),
\]
where $[\cdot\|\cdot]$ denotes channel-wise concatenation and $\sigma$ is the element-wise sigmoid.

\subsubsection{Fused Representation}
The fused feature map $F_f$ is computed as a per-pixel interpolation of $F_r$ and $F_c$:
\[
F_f = W \odot F_r + (1 - W) \odot F_c,
\]
where $\odot$ denotes broadcasted element-wise multiplication. In regions where the RIC branch’s rotation-equivariant features are more informative, $W$ approaches 1; elsewhere, the standard CNN features dominate.

\subsubsection{Auxiliary Supervision}
To encourage each branch to learn discriminative features, we attach auxiliary classifiers $h_r(\cdot)$ and $h_c(\cdot)$ to $F_r$ and $F_c$, respectively. The final classification is performed by $h_f(F_f)$. The overall loss is:
\[
\mathcal{L} = \mathcal{L}_{\mathrm{CE}}\bigl(h_f(F_f), y\bigr) + \lambda_r \, \mathcal{L}_{\mathrm{CE}}\bigl(h_r(F_r), y\bigr) + \lambda_c \, \mathcal{L}_{\mathrm{CE}}\bigl(h_c(F_c), y\bigr),
\]
where $\mathcal{L}_{\mathrm{CE}}$ is the cross-entropy loss, $y$ the ground-truth label, and $\lambda_r,\lambda_c$ balance the auxiliary terms.

This fusion mechanism dynamically extracts rotational information from the mismatch between equivariant and variant feature maps, leading to balanced performance on both rotated and non-rotated inputs.